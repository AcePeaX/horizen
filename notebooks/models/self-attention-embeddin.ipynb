{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention embedding table\n",
    "\n",
    "Adding route to custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "dirname = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts/lib\"))\n",
    "sys.path.append(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.compile import compileFolder\n",
    "from utils.tokenizer import CharTokenizer, END_CHAR\n",
    "from utils.datasets import TextChunksDataset, split_dataset, get_batch, estimate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module helps to quickly save the weights and load them\n",
    "from transformers import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max block size (also known as max context) [in tokens]\n",
    "block_size = 8\n",
    "\n",
    "# How much does the test/validation set represent of the total data\n",
    "test_train_split_ratio = 0.1\n",
    "\n",
    "# Number of embedding\n",
    "n_embd = 32\n",
    "\n",
    "# Device (gpu or cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "raw_data = compileFolder('tate')\n",
    "\n",
    "# Creating the tokenizer\n",
    "tokenizer = CharTokenizer(raw_data)\n",
    "\n",
    "# Tokenizing and creating the dataset object\n",
    "data = TextChunksDataset(raw_data, block_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the self attention block\n",
    "\n",
    "We take almost the same structure as the base-embedding structure\n",
    "\n",
    "> Note: starting from now, we're going to use `cuda` when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int | CharTokenizer | TextChunksDataset, n_embd, device=device, context_size=None):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        self.device = device\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd, device=self.device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LeL:havOnUnmEVLM:bRwFt dM0c?cDtLv-?oJ=?VaRJ7\"2M20AM\n",
      "fn8bFyZ:?m7N0na0wm'qKeL\n",
      "8T?B?9!ib:6Zbp2A:%tsaM8\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(89)\n",
    "m = BigramLanguageModel(train_data, n_embd)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is random characters\n",
    "\n",
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# verbose\n",
    "show_loss_each_epoch = 500\n",
    "\n",
    "def train(optimizer, num_epochs=num_epochs, loss_verbose_interval=show_loss_each_epoch):\n",
    "    for steps in range(num_epochs):\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(train_data, batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (steps+1)%loss_verbose_interval==0:\n",
    "            losses = estimate_loss(m, train_data, test_data, batch_size=batch_size)\n",
    "            print(f\"step {steps+1}: train loss {losses ['train']:.4f}, val loss {losses ['val']:.4f}\")\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.6314, val loss 2.7112\n",
      "step 1000: train loss 2.5292, val loss 2.5621\n",
      "step 1500: train loss 2.4844, val loss 2.5606\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hin theanigrst yo t joine whe, meat\n",
      "\n",
      "\n",
      "\n",
      "s. n aacy thoreye thande.ove toutonce i k okered?\n",
      "the m hoe c\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still the same result as the base model\n",
    "\n",
    "\n",
    "## The mathematical trick to self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "\n",
    "torch.manual_seed(198)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3597,  0.1501],\n",
       "        [ 0.3383,  0.7864],\n",
       "        [ 0.3464,  0.5391],\n",
       "        [ 0.0438,  0.4631],\n",
       "        [ 0.0989,  0.1779],\n",
       "        [ 0.2658,  0.2987],\n",
       "        [-0.0105,  0.2283],\n",
       "        [ 0.0798,  0.2066]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b, t] = the mean of x[b, i] with i<=t\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second version (using Softmax)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3597,  0.1501],\n",
       "        [ 0.3383,  0.7864],\n",
       "        [ 0.3464,  0.5391],\n",
       "        [ 0.0438,  0.4631],\n",
       "        [ 0.0989,  0.1779],\n",
       "        [ 0.2658,  0.2987],\n",
       "        [-0.0105,  0.2283],\n",
       "        [ 0.0798,  0.2066]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the same tensor\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same result\n",
    "\n",
    "### Implementing an attention head\n",
    "Now we're going to have a third version: the attention head.\n",
    "\n",
    "We introduce first this new parameter: the head size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head size\n",
    "head_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4099, 0.5901, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3780, 0.2279, 0.3942, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1302, 0.5305, 0.1159, 0.2234, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2031, 0.0725, 0.2210, 0.0464, 0.4569, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1195, 0.0402, 0.1308, 0.3458, 0.3329, 0.0308, 0.0000, 0.0000],\n",
       "        [0.0555, 0.5743, 0.0458, 0.0614, 0.0072, 0.2289, 0.0269, 0.0000],\n",
       "        [0.1148, 0.0417, 0.1248, 0.1055, 0.2782, 0.0637, 0.1468, 0.1245]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the weights are not uniform in a row, rather have different values.\n",
    "\n",
    "So we've implemented the keys, the queries, now we'll implement the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias=False) # Same linear structure as the key and query linear models\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2252,  0.0816, -0.0861, -0.0980, -0.1490, -0.0824, -0.2695,  0.0722,\n",
       "         0.1332, -0.1667, -0.0388, -0.2477, -0.0050,  0.1838, -0.0065,  0.1206],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,0] # The value of the first token for the first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variance stability purposes, `wei` tensor needs to be divided by $\\sqrt{\\text{head\\_size}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * (head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `Head` class that implements a single head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # Compute attention score ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v     # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out        # (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's redefine `BigramLanguageModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int | CharTokenizer | TextChunksDataset, n_embd, device=device, context_size=None, head_size=16):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        self.device = device\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd, device=self.device)\n",
    "        self.sa_head = Head(n_embd, n_embd, vocab_size)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gth2C-riwP\"wSCi0vOy%C…XsT5/\"ftn1\n",
      "8!!*aG*T8eP$4\n",
      "mUxCcYTC1H/Wiy\n",
      "Bl5wX\n",
      "-6\n",
      "*Q'*4W/5HC9XSq4CKC*…96>oC2Z…A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = BigramLanguageModel(train_data, n_embd)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.5529, val loss 2.6305\n",
      "step 1000: train loss 2.5604, val loss 2.5830\n",
      "step 1500: train loss 2.5226, val loss 2.5723\n",
      "step 2000: train loss 2.5229, val loss 2.5483\n",
      "step 2500: train loss 2.5096, val loss 2.5469\n",
      "step 3000: train loss 2.4930, val loss 2.5002\n",
      "step 3500: train loss 2.4778, val loss 2.5315\n",
      "step 4000: train loss 2.4406, val loss 2.5140\n",
      "step 4500: train loss 2.4527, val loss 2.4918\n",
      "step 5000: train loss 2.4408, val loss 2.4792\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I't\n",
      "y.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Esiing mt thel, wigr\n",
      "erandoe ined to ulifele ouere\n",
      "\n",
      "Os fomd Qe't I d hemlbe ase ple kou u\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet very good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
