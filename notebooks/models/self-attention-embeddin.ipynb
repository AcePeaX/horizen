{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention embedding table\n",
    "\n",
    "Adding route to custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "dirname = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts/lib\"))\n",
    "sys.path.append(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.compile import compileFolder\n",
    "from utils.tokenizer import CharTokenizer, END_CHAR\n",
    "from utils.datasets import TextChunksDataset, split_dataset, get_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module helps to quickly save the weights and load them\n",
    "from transformers import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max block size (also known as max context) [in tokens]\n",
    "block_size = 8\n",
    "\n",
    "# How much does the test/validation set represent of the total data\n",
    "test_train_split_ratio = 0.1\n",
    "\n",
    "# Number of embedding\n",
    "n_embd = 32\n",
    "\n",
    "# Device (gpu or cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "raw_data = compileFolder('tate')\n",
    "\n",
    "# Creating the tokenizer\n",
    "tokenizer = CharTokenizer(raw_data)\n",
    "\n",
    "# Tokenizing and creating the dataset object\n",
    "data = TextChunksDataset(raw_data, block_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the self attention block\n",
    "\n",
    "We take almost the same structure as the base-embedding structure\n",
    "\n",
    "> Note: starting from now, we're going to use `cuda` when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int | CharTokenizer | TextChunksDataset, n_embd, device=device, context_size=None):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        self.device = device\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd, device=self.device)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LeL:havOnUnmEVLM:bRwFt dM0c?cDtLv-?oJ=?VaRJ7\"2M20AM\n",
      "fn8bFyZ:?m7N0na0wm'qKeL\n",
      "8T?B?9!ib:6Zbp2A:%tsaM8\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(89)\n",
    "m = BigramLanguageModel(train_data, n_embd)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is random characters\n",
    "\n",
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# verbose\n",
    "show_loss_each_epoch = 10\n",
    "\n",
    "def train(optimizer, num_epochs=num_epochs):\n",
    "    for steps in range(num_epochs):\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(train_data, batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (steps+1)%show_loss_each_epoch==0:\n",
    "            print('loss :',loss.item())\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 4.564981937408447\n",
      "loss : 4.342898845672607\n",
      "loss : 4.289647102355957\n",
      "loss : 4.166916370391846\n",
      "loss : 3.926363468170166\n",
      "loss : 3.88124942779541\n",
      "loss : 3.672184705734253\n",
      "loss : 3.659520387649536\n",
      "loss : 3.497307300567627\n",
      "loss : 3.4887804985046387\n",
      "loss : 3.4159581661224365\n",
      "loss : 3.3326728343963623\n",
      "loss : 3.1581273078918457\n",
      "loss : 3.283029794692993\n",
      "loss : 3.0645952224731445\n",
      "loss : 3.0826728343963623\n",
      "loss : 3.0945539474487305\n",
      "loss : 2.938164710998535\n",
      "loss : 3.0291380882263184\n",
      "loss : 2.8865647315979004\n",
      "loss : 3.0385732650756836\n",
      "loss : 2.7839837074279785\n",
      "loss : 2.8570516109466553\n",
      "loss : 2.7800652980804443\n",
      "loss : 3.0109987258911133\n",
      "loss : 2.7849817276000977\n",
      "loss : 2.7034881114959717\n",
      "loss : 2.6601319313049316\n",
      "loss : 2.9246671199798584\n",
      "loss : 2.7791905403137207\n",
      "loss : 2.710498571395874\n",
      "loss : 2.7385547161102295\n",
      "loss : 2.6902031898498535\n",
      "loss : 2.615878105163574\n",
      "loss : 2.616091251373291\n",
      "loss : 2.9033002853393555\n",
      "loss : 2.735739231109619\n",
      "loss : 2.7156715393066406\n",
      "loss : 2.7572851181030273\n",
      "loss : 2.640770196914673\n",
      "loss : 2.6971688270568848\n",
      "loss : 2.701404094696045\n",
      "loss : 2.5857911109924316\n",
      "loss : 2.6507980823516846\n",
      "loss : 2.7893457412719727\n",
      "loss : 2.6208083629608154\n",
      "loss : 2.6114532947540283\n",
      "loss : 2.5206596851348877\n",
      "loss : 2.6591224670410156\n",
      "loss : 2.6139028072357178\n",
      "loss : 2.665184259414673\n",
      "loss : 2.516653299331665\n",
      "loss : 2.505091667175293\n",
      "loss : 2.584104061126709\n",
      "loss : 2.510033369064331\n",
      "loss : 2.3997228145599365\n",
      "loss : 2.527228593826294\n",
      "loss : 2.61383318901062\n",
      "loss : 2.580496072769165\n",
      "loss : 2.8379786014556885\n",
      "loss : 2.4749908447265625\n",
      "loss : 2.5201385021209717\n",
      "loss : 2.616307020187378\n",
      "loss : 2.452780246734619\n",
      "loss : 2.652512311935425\n",
      "loss : 2.550776958465576\n",
      "loss : 2.630864381790161\n",
      "loss : 2.5537362098693848\n",
      "loss : 2.53489089012146\n",
      "loss : 2.527585506439209\n",
      "loss : 2.4587996006011963\n",
      "loss : 2.417848587036133\n",
      "loss : 2.4872353076934814\n",
      "loss : 2.626502752304077\n",
      "loss : 2.581058979034424\n",
      "loss : 2.662027597427368\n",
      "loss : 2.600154399871826\n",
      "loss : 2.4763951301574707\n",
      "loss : 2.555872917175293\n",
      "loss : 2.5107662677764893\n",
      "loss : 2.503659963607788\n",
      "loss : 2.479887008666992\n",
      "loss : 2.6281938552856445\n",
      "loss : 2.4997968673706055\n",
      "loss : 2.3356761932373047\n",
      "loss : 2.5637454986572266\n",
      "loss : 2.5277023315429688\n",
      "loss : 2.552220582962036\n",
      "loss : 2.5997092723846436\n",
      "loss : 2.5070292949676514\n",
      "loss : 2.580091714859009\n",
      "loss : 2.3602242469787598\n",
      "loss : 2.4978508949279785\n",
      "loss : 2.408397674560547\n",
      "loss : 2.521909236907959\n",
      "loss : 2.524845838546753\n",
      "loss : 2.4073433876037598\n",
      "loss : 2.4952962398529053\n",
      "loss : 2.5773913860321045\n",
      "loss : 2.6473705768585205\n",
      "loss : 2.64095139503479\n",
      "loss : 2.5872294902801514\n",
      "loss : 2.5738096237182617\n",
      "loss : 2.452903985977173\n",
      "loss : 2.6512584686279297\n",
      "loss : 2.7575018405914307\n",
      "loss : 2.6819639205932617\n",
      "loss : 2.599670886993408\n",
      "loss : 2.587679386138916\n",
      "loss : 2.468189001083374\n",
      "loss : 2.438734769821167\n",
      "loss : 2.437289237976074\n",
      "loss : 2.686218023300171\n",
      "loss : 2.701352119445801\n",
      "loss : 2.5412468910217285\n",
      "loss : 2.6047019958496094\n",
      "loss : 2.5572237968444824\n",
      "loss : 2.5611398220062256\n",
      "loss : 2.557234764099121\n",
      "loss : 2.657270908355713\n",
      "loss : 2.5144546031951904\n",
      "loss : 2.456679582595825\n",
      "loss : 2.540370464324951\n",
      "loss : 2.688206911087036\n",
      "loss : 2.3336241245269775\n",
      "loss : 2.375135898590088\n",
      "loss : 2.535330295562744\n",
      "loss : 2.5126540660858154\n",
      "loss : 2.5232419967651367\n",
      "loss : 2.5968735218048096\n",
      "loss : 2.50339412689209\n",
      "loss : 2.424610137939453\n",
      "loss : 2.3706424236297607\n",
      "loss : 2.629706859588623\n",
      "loss : 2.4699416160583496\n",
      "loss : 2.3903861045837402\n",
      "loss : 2.5007424354553223\n",
      "loss : 2.4680325984954834\n",
      "loss : 2.437715768814087\n",
      "loss : 2.5762863159179688\n",
      "loss : 2.5181915760040283\n",
      "loss : 2.4744672775268555\n",
      "loss : 2.335167407989502\n",
      "loss : 2.4703330993652344\n",
      "loss : 2.4746623039245605\n",
      "loss : 2.4823050498962402\n",
      "loss : 2.5041544437408447\n",
      "loss : 2.639913320541382\n",
      "loss : 2.5303311347961426\n",
      "loss : 2.540329694747925\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Oheve  y angowhay lerly I anoura g te ubulitinst wea youtpe ththin igrst yo t joine war, meat\n",
      "\n",
      "\n",
      "\n",
      "s\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still the same result as the base model\n",
    "\n",
    "\n",
    "## The mathematical trick to self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "\n",
    "torch.manual_seed(198)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3597,  0.1501],\n",
       "        [ 0.3383,  0.7864],\n",
       "        [ 0.3464,  0.5391],\n",
       "        [ 0.0438,  0.4631],\n",
       "        [ 0.0989,  0.1779],\n",
       "        [ 0.2658,  0.2987],\n",
       "        [-0.0105,  0.2283],\n",
       "        [ 0.0798,  0.2066]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b, t] = the mean of x[b, i] with i<=t\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second version (using Softmax)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3597,  0.1501],\n",
       "        [ 0.3383,  0.7864],\n",
       "        [ 0.3464,  0.5391],\n",
       "        [ 0.0438,  0.4631],\n",
       "        [ 0.0989,  0.1779],\n",
       "        [ 0.2658,  0.2987],\n",
       "        [-0.0105,  0.2283],\n",
       "        [ 0.0798,  0.2066]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the same tensor\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same result\n",
    "\n",
    "### Implementing an attention head\n",
    "Now we're going to have a third version: the attention head.\n",
    "\n",
    "We introduce first this new parameter: the head size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head size\n",
    "head_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4099, 0.5901, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3780, 0.2279, 0.3942, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1302, 0.5305, 0.1159, 0.2234, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2031, 0.0725, 0.2210, 0.0464, 0.4569, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1195, 0.0402, 0.1308, 0.3458, 0.3329, 0.0308, 0.0000, 0.0000],\n",
       "        [0.0555, 0.5743, 0.0458, 0.0614, 0.0072, 0.2289, 0.0269, 0.0000],\n",
       "        [0.1148, 0.0417, 0.1248, 0.1055, 0.2782, 0.0637, 0.1468, 0.1245]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the weights are not uniform in a row, rather have different values.\n",
    "\n",
    "So we've implemented the keys, the queries, now we'll implement the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias=False) # Same linear structure as the key and query linear models\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2252,  0.0816, -0.0861, -0.0980, -0.1490, -0.0824, -0.2695,  0.0722,\n",
       "         0.1332, -0.1667, -0.0388, -0.2477, -0.0050,  0.1838, -0.0065,  0.1206],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,0] # The value of the first token for the first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variance stability purposes, `wei` tensor needs to be divided by $\\sqrt{\\text{head\\_size}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * (head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `Head` class that implements a single head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # Compute attention score ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v     # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out        # (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's redefine `BigramLanguageModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int | CharTokenizer | TextChunksDataset, n_embd, device=device, context_size=None, head_size=16):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        self.device = device\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd, device=self.device)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd, device=self.device)\n",
    "        self.sa_head = Head(n_embd, n_embd, vocab_size)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, device=self.device)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=self.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gth2C-riwP\"wSCi0vOy%C…XsT5/\"ftn1\n",
      "8!!*aG*T8eP$4\n",
      "mUxCcYTC1H/Wiy\n",
      "Bl5wX\n",
      "-6\n",
      "*Q'*4W/5HC9XSq4CKC*…96>oC2Z…A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = BigramLanguageModel(train_data, n_embd)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 4.447386741638184\n",
      "loss : 4.421143531799316\n",
      "loss : 4.351117134094238\n",
      "loss : 4.393448352813721\n",
      "loss : 4.375442028045654\n",
      "loss : 4.336314678192139\n",
      "loss : 4.288149356842041\n",
      "loss : 4.264897346496582\n",
      "loss : 4.263428211212158\n",
      "loss : 4.239197731018066\n",
      "loss : 4.180443286895752\n",
      "loss : 4.160712242126465\n",
      "loss : 4.150298595428467\n",
      "loss : 4.129261016845703\n",
      "loss : 4.094697952270508\n",
      "loss : 4.087104797363281\n",
      "loss : 4.062117099761963\n",
      "loss : 3.9753551483154297\n",
      "loss : 3.968000888824463\n",
      "loss : 3.92964506149292\n",
      "loss : 3.8719117641448975\n",
      "loss : 3.9430956840515137\n",
      "loss : 3.8390052318573\n",
      "loss : 3.8218882083892822\n",
      "loss : 3.7549421787261963\n",
      "loss : 3.7504026889801025\n",
      "loss : 3.759014368057251\n",
      "loss : 3.6938560009002686\n",
      "loss : 3.599816083908081\n",
      "loss : 3.6268932819366455\n",
      "loss : 3.5400853157043457\n",
      "loss : 3.522675037384033\n",
      "loss : 3.5024819374084473\n",
      "loss : 3.520388126373291\n",
      "loss : 3.523282289505005\n",
      "loss : 3.50000262260437\n",
      "loss : 3.451000690460205\n",
      "loss : 3.400134325027466\n",
      "loss : 3.378018379211426\n",
      "loss : 3.4012668132781982\n",
      "loss : 3.367363929748535\n",
      "loss : 3.4684367179870605\n",
      "loss : 3.3607048988342285\n",
      "loss : 3.2568247318267822\n",
      "loss : 3.42276668548584\n",
      "loss : 3.265434741973877\n",
      "loss : 3.260558843612671\n",
      "loss : 3.1992006301879883\n",
      "loss : 3.206899642944336\n",
      "loss : 3.289680004119873\n",
      "loss : 3.1376357078552246\n",
      "loss : 3.2877025604248047\n",
      "loss : 3.4252731800079346\n",
      "loss : 3.255066156387329\n",
      "loss : 3.1039528846740723\n",
      "loss : 3.265092611312866\n",
      "loss : 3.2940497398376465\n",
      "loss : 3.095179796218872\n",
      "loss : 3.3207907676696777\n",
      "loss : 3.228969097137451\n",
      "loss : 3.2033870220184326\n",
      "loss : 3.1590750217437744\n",
      "loss : 3.166382074356079\n",
      "loss : 3.1683239936828613\n",
      "loss : 3.2413458824157715\n",
      "loss : 3.0812830924987793\n",
      "loss : 3.1744065284729004\n",
      "loss : 3.137044668197632\n",
      "loss : 3.0833358764648438\n",
      "loss : 3.111790895462036\n",
      "loss : 3.2163288593292236\n",
      "loss : 3.276272773742676\n",
      "loss : 3.135380744934082\n",
      "loss : 3.1405742168426514\n",
      "loss : 3.256377935409546\n",
      "loss : 3.072600841522217\n",
      "loss : 3.0709590911865234\n",
      "loss : 3.1907119750976562\n",
      "loss : 3.0831146240234375\n",
      "loss : 3.204235076904297\n",
      "loss : 3.098026752471924\n",
      "loss : 3.1244964599609375\n",
      "loss : 3.177637815475464\n",
      "loss : 3.1274960041046143\n",
      "loss : 3.1205971240997314\n",
      "loss : 3.2252705097198486\n",
      "loss : 3.1680188179016113\n",
      "loss : 3.257607936859131\n",
      "loss : 3.163490056991577\n",
      "loss : 3.0758256912231445\n",
      "loss : 3.105707883834839\n",
      "loss : 3.083808660507202\n",
      "loss : 2.978487491607666\n",
      "loss : 3.0869944095611572\n",
      "loss : 3.106282949447632\n",
      "loss : 3.059257984161377\n",
      "loss : 3.045987844467163\n",
      "loss : 3.2212674617767334\n",
      "loss : 3.055985689163208\n",
      "loss : 3.017115831375122\n",
      "loss : 3.10821533203125\n",
      "loss : 3.125147819519043\n",
      "loss : 3.2439770698547363\n",
      "loss : 3.039606809616089\n",
      "loss : 3.0120251178741455\n",
      "loss : 2.9425737857818604\n",
      "loss : 3.172785520553589\n",
      "loss : 3.0813851356506348\n",
      "loss : 3.1917977333068848\n",
      "loss : 3.0609641075134277\n",
      "loss : 3.1998467445373535\n",
      "loss : 3.1113626956939697\n",
      "loss : 3.0096607208251953\n",
      "loss : 3.2369167804718018\n",
      "loss : 2.9968879222869873\n",
      "loss : 3.037834882736206\n",
      "loss : 3.1000657081604004\n",
      "loss : 3.055962085723877\n",
      "loss : 2.9970083236694336\n",
      "loss : 2.921231985092163\n",
      "loss : 3.2055535316467285\n",
      "loss : 2.8774333000183105\n",
      "loss : 3.1762688159942627\n",
      "loss : 3.126372814178467\n",
      "loss : 3.042665958404541\n",
      "loss : 3.110574245452881\n",
      "loss : 2.9920432567596436\n",
      "loss : 2.9480173587799072\n",
      "loss : 3.09574294090271\n",
      "loss : 2.993511915206909\n",
      "loss : 3.0579559803009033\n",
      "loss : 2.9660468101501465\n",
      "loss : 3.1180713176727295\n",
      "loss : 3.028079032897949\n",
      "loss : 3.017195224761963\n",
      "loss : 3.053598642349243\n",
      "loss : 2.975640058517456\n",
      "loss : 3.129544258117676\n",
      "loss : 3.0254011154174805\n",
      "loss : 3.024150848388672\n",
      "loss : 2.9826910495758057\n",
      "loss : 2.99322772026062\n",
      "loss : 3.033235549926758\n",
      "loss : 2.948009967803955\n",
      "loss : 2.9583699703216553\n",
      "loss : 2.9440979957580566\n",
      "loss : 2.909721851348877\n",
      "loss : 2.9688825607299805\n",
      "loss : 2.914257526397705\n",
      "loss : 3.2020297050476074\n",
      "loss : 3.100713014602661\n",
      "loss : 2.967373847961426\n",
      "loss : 2.9314799308776855\n",
      "loss : 2.9083621501922607\n",
      "loss : 2.9730536937713623\n",
      "loss : 2.992415189743042\n",
      "loss : 3.0275206565856934\n",
      "loss : 3.0340170860290527\n",
      "loss : 2.9303438663482666\n",
      "loss : 3.054391622543335\n",
      "loss : 2.9496984481811523\n",
      "loss : 2.920443058013916\n",
      "loss : 2.93436861038208\n",
      "loss : 2.9726674556732178\n",
      "loss : 3.014636754989624\n",
      "loss : 2.9654312133789062\n",
      "loss : 2.9059946537017822\n",
      "loss : 3.0420918464660645\n",
      "loss : 2.9664270877838135\n",
      "loss : 3.0423614978790283\n",
      "loss : 2.90767502784729\n",
      "loss : 2.8249173164367676\n",
      "loss : 2.90301251411438\n",
      "loss : 3.062836170196533\n",
      "loss : 2.9475913047790527\n",
      "loss : 2.7760860919952393\n",
      "loss : 2.8564260005950928\n",
      "loss : 2.899630069732666\n",
      "loss : 2.9060471057891846\n",
      "loss : 3.0751545429229736\n",
      "loss : 2.936002016067505\n",
      "loss : 2.9094176292419434\n",
      "loss : 2.8805041313171387\n",
      "loss : 3.049208879470825\n",
      "loss : 2.932610034942627\n",
      "loss : 2.9000229835510254\n",
      "loss : 3.038966178894043\n",
      "loss : 2.974715232849121\n",
      "loss : 2.923100233078003\n",
      "loss : 2.9462661743164062\n",
      "loss : 2.882936477661133\n",
      "loss : 2.988502264022827\n",
      "loss : 2.963649272918701\n",
      "loss : 2.998192310333252\n",
      "loss : 2.9260244369506836\n",
      "loss : 2.943671703338623\n",
      "loss : 2.97375226020813\n",
      "loss : 2.841660976409912\n",
      "loss : 2.7925829887390137\n",
      "loss : 3.134472131729126\n",
      "loss : 2.823533535003662\n",
      "loss : 3.035111904144287\n",
      "loss : 2.980182647705078\n",
      "loss : 2.9328181743621826\n",
      "loss : 2.8865089416503906\n",
      "loss : 2.8909430503845215\n",
      "loss : 2.92332124710083\n",
      "loss : 2.905197858810425\n",
      "loss : 3.003258466720581\n",
      "loss : 2.9832425117492676\n",
      "loss : 2.930734872817993\n",
      "loss : 2.7744922637939453\n",
      "loss : 2.845930337905884\n",
      "loss : 2.829418897628784\n",
      "loss : 2.943983554840088\n",
      "loss : 3.121433973312378\n",
      "loss : 2.9771037101745605\n",
      "loss : 2.793579339981079\n",
      "loss : 2.880789041519165\n",
      "loss : 3.0657413005828857\n",
      "loss : 2.827336072921753\n",
      "loss : 2.9622714519500732\n",
      "loss : 2.814809799194336\n",
      "loss : 2.770296812057495\n",
      "loss : 2.9282681941986084\n",
      "loss : 2.9317715167999268\n",
      "loss : 2.7845911979675293\n",
      "loss : 2.864691972732544\n",
      "loss : 2.8853719234466553\n",
      "loss : 2.8817222118377686\n",
      "loss : 2.8305554389953613\n",
      "loss : 2.8079543113708496\n",
      "loss : 2.968848943710327\n",
      "loss : 2.7014153003692627\n",
      "loss : 2.7821435928344727\n",
      "loss : 2.841228723526001\n",
      "loss : 2.826589822769165\n",
      "loss : 2.9915659427642822\n",
      "loss : 2.8095617294311523\n",
      "loss : 2.6826462745666504\n",
      "loss : 3.041522741317749\n",
      "loss : 2.7326102256774902\n",
      "loss : 2.920907735824585\n",
      "loss : 2.918586492538452\n",
      "loss : 2.723621129989624\n",
      "loss : 2.8033368587493896\n",
      "loss : 2.81630802154541\n",
      "loss : 2.808231830596924\n",
      "loss : 2.814368486404419\n",
      "loss : 2.7131998538970947\n",
      "loss : 2.941716432571411\n",
      "loss : 2.833400011062622\n",
      "loss : 2.704151153564453\n",
      "loss : 2.786102771759033\n",
      "loss : 2.727388858795166\n",
      "loss : 2.8411753177642822\n",
      "loss : 2.919563055038452\n",
      "loss : 2.9158859252929688\n",
      "loss : 2.817945957183838\n",
      "loss : 2.824326515197754\n",
      "loss : 2.792229175567627\n",
      "loss : 2.803063154220581\n",
      "loss : 2.804751396179199\n",
      "loss : 2.5924575328826904\n",
      "loss : 2.6849050521850586\n",
      "loss : 2.6084482669830322\n",
      "loss : 2.924642562866211\n",
      "loss : 2.779186487197876\n",
      "loss : 2.767526626586914\n",
      "loss : 2.880877733230591\n",
      "loss : 2.8419690132141113\n",
      "loss : 2.6299383640289307\n",
      "loss : 2.8510773181915283\n",
      "loss : 2.701974630355835\n",
      "loss : 2.879060983657837\n",
      "loss : 2.7082133293151855\n",
      "loss : 2.771585702896118\n",
      "loss : 2.7949371337890625\n",
      "loss : 2.790548086166382\n",
      "loss : 2.706789493560791\n",
      "loss : 2.7742881774902344\n",
      "loss : 2.8091847896575928\n",
      "loss : 2.871744155883789\n",
      "loss : 2.980497121810913\n",
      "loss : 2.844001293182373\n",
      "loss : 2.75004506111145\n",
      "loss : 2.7797842025756836\n",
      "loss : 2.7649641036987305\n",
      "loss : 2.7568178176879883\n",
      "loss : 2.6933999061584473\n",
      "loss : 2.825120210647583\n",
      "loss : 2.6097068786621094\n",
      "loss : 2.8770008087158203\n",
      "loss : 3.0076448917388916\n",
      "loss : 2.8167524337768555\n",
      "loss : 2.7995667457580566\n",
      "loss : 2.7524614334106445\n",
      "loss : 2.818464994430542\n",
      "loss : 2.7722063064575195\n",
      "loss : 2.6154704093933105\n",
      "loss : 2.5857667922973633\n",
      "loss : 2.8590335845947266\n",
      "loss : 2.8483731746673584\n",
      "loss : 2.7609148025512695\n",
      "loss : 2.642031192779541\n",
      "loss : 2.592432975769043\n",
      "loss : 2.680955410003662\n",
      "loss : 2.6091859340667725\n",
      "loss : 2.5729050636291504\n",
      "loss : 2.639573812484741\n",
      "loss : 2.7351717948913574\n",
      "loss : 3.012782335281372\n",
      "loss : 2.764068841934204\n",
      "loss : 2.7602803707122803\n",
      "loss : 2.759145498275757\n",
      "loss : 2.7538905143737793\n",
      "loss : 2.793524980545044\n",
      "loss : 2.769444227218628\n",
      "loss : 2.6898279190063477\n",
      "loss : 2.630949020385742\n",
      "loss : 2.7334647178649902\n",
      "loss : 2.7113757133483887\n",
      "loss : 2.638364791870117\n",
      "loss : 2.6533734798431396\n",
      "loss : 2.729311943054199\n",
      "loss : 2.573667049407959\n",
      "loss : 2.7139101028442383\n",
      "loss : 2.8006787300109863\n",
      "loss : 2.602583646774292\n",
      "loss : 2.6999433040618896\n",
      "loss : 2.7458715438842773\n",
      "loss : 2.6716675758361816\n",
      "loss : 2.774608850479126\n",
      "loss : 2.781919479370117\n",
      "loss : 2.917633533477783\n",
      "loss : 2.7125484943389893\n",
      "loss : 2.8063971996307373\n",
      "loss : 2.663004159927368\n",
      "loss : 2.741011142730713\n",
      "loss : 2.6670374870300293\n",
      "loss : 2.7680015563964844\n",
      "loss : 2.7251203060150146\n",
      "loss : 2.7550854682922363\n",
      "loss : 2.696619749069214\n",
      "loss : 2.670778751373291\n",
      "loss : 2.665822982788086\n",
      "loss : 2.6533656120300293\n",
      "loss : 2.65836501121521\n",
      "loss : 2.761606454849243\n",
      "loss : 2.7428319454193115\n",
      "loss : 2.755504846572876\n",
      "loss : 2.6376805305480957\n",
      "loss : 2.6293718814849854\n",
      "loss : 2.6953091621398926\n",
      "loss : 2.7292206287384033\n",
      "loss : 2.6453959941864014\n",
      "loss : 2.8000969886779785\n",
      "loss : 2.684654712677002\n",
      "loss : 2.6785383224487305\n",
      "loss : 2.672001838684082\n",
      "loss : 2.8009331226348877\n",
      "loss : 2.6832616329193115\n",
      "loss : 2.746450185775757\n",
      "loss : 2.6642138957977295\n",
      "loss : 2.6916873455047607\n",
      "loss : 2.97713041305542\n",
      "loss : 2.653015375137329\n",
      "loss : 2.7775213718414307\n",
      "loss : 2.750100612640381\n",
      "loss : 2.6542258262634277\n",
      "loss : 2.7978973388671875\n",
      "loss : 2.721398115158081\n",
      "loss : 2.716322898864746\n",
      "loss : 2.689272165298462\n",
      "loss : 2.875401496887207\n",
      "loss : 2.5110573768615723\n",
      "loss : 2.647357940673828\n",
      "loss : 2.7676186561584473\n",
      "loss : 2.634406089782715\n",
      "loss : 2.7705724239349365\n",
      "loss : 2.6820011138916016\n",
      "loss : 2.634122610092163\n",
      "loss : 2.5681397914886475\n",
      "loss : 2.770697593688965\n",
      "loss : 2.6503865718841553\n",
      "loss : 2.65169358253479\n",
      "loss : 2.609506845474243\n",
      "loss : 2.6485238075256348\n",
      "loss : 2.74165940284729\n",
      "loss : 2.8274567127227783\n",
      "loss : 2.6640052795410156\n",
      "loss : 2.6608083248138428\n",
      "loss : 2.7936437129974365\n",
      "loss : 2.7259364128112793\n",
      "loss : 2.6807737350463867\n",
      "loss : 2.7799015045166016\n",
      "loss : 2.5701780319213867\n",
      "loss : 2.451542615890503\n",
      "loss : 2.866297960281372\n",
      "loss : 2.6415512561798096\n",
      "loss : 2.638798952102661\n",
      "loss : 2.6840479373931885\n",
      "loss : 2.6071717739105225\n",
      "loss : 2.6029727458953857\n",
      "loss : 2.658236026763916\n",
      "loss : 2.564452886581421\n",
      "loss : 2.6323482990264893\n",
      "loss : 2.7141966819763184\n",
      "loss : 2.5618648529052734\n",
      "loss : 2.670713186264038\n",
      "loss : 2.744901657104492\n",
      "loss : 2.786707639694214\n",
      "loss : 2.4401633739471436\n",
      "loss : 2.5924232006073\n",
      "loss : 2.6735687255859375\n",
      "loss : 2.7643990516662598\n",
      "loss : 2.5201776027679443\n",
      "loss : 2.8546276092529297\n",
      "loss : 2.620307683944702\n",
      "loss : 2.5653202533721924\n",
      "loss : 2.6367578506469727\n",
      "loss : 2.7365734577178955\n",
      "loss : 2.6213457584381104\n",
      "loss : 2.732339859008789\n",
      "loss : 2.5629220008850098\n",
      "loss : 2.6646437644958496\n",
      "loss : 2.752129316329956\n",
      "loss : 2.572606086730957\n",
      "loss : 2.803011417388916\n",
      "loss : 2.603681802749634\n",
      "loss : 2.619511127471924\n",
      "loss : 2.5608577728271484\n",
      "loss : 2.6284749507904053\n",
      "loss : 2.6903889179229736\n",
      "loss : 2.630192518234253\n",
      "loss : 2.652261257171631\n",
      "loss : 2.7089037895202637\n",
      "loss : 2.5738155841827393\n",
      "loss : 2.7278900146484375\n",
      "loss : 2.6722147464752197\n",
      "loss : 2.7663474082946777\n",
      "loss : 2.5937230587005615\n",
      "loss : 2.5227320194244385\n",
      "loss : 2.863356113433838\n",
      "loss : 2.6537857055664062\n",
      "loss : 2.4996674060821533\n",
      "loss : 2.8514230251312256\n",
      "loss : 2.617971420288086\n",
      "loss : 2.609816074371338\n",
      "loss : 2.6831512451171875\n",
      "loss : 2.6456503868103027\n",
      "loss : 2.579728841781616\n",
      "loss : 2.7263944149017334\n",
      "loss : 2.687742233276367\n",
      "loss : 2.588132619857788\n",
      "loss : 2.5135998725891113\n",
      "loss : 2.8167502880096436\n",
      "loss : 2.5468549728393555\n",
      "loss : 2.543273448944092\n",
      "loss : 2.858588695526123\n",
      "loss : 2.645996570587158\n",
      "loss : 2.7762696743011475\n",
      "loss : 2.599900007247925\n",
      "loss : 2.803767681121826\n",
      "loss : 2.6598567962646484\n",
      "loss : 2.533292770385742\n",
      "loss : 2.558743715286255\n",
      "loss : 2.49935245513916\n",
      "loss : 2.4489312171936035\n",
      "loss : 2.6246895790100098\n",
      "loss : 2.50069522857666\n",
      "loss : 2.8545846939086914\n",
      "loss : 2.6244804859161377\n",
      "loss : 2.5511021614074707\n",
      "loss : 2.804753065109253\n",
      "loss : 2.5616700649261475\n",
      "loss : 2.566408157348633\n",
      "loss : 2.617030143737793\n",
      "loss : 2.5414671897888184\n",
      "loss : 2.530120372772217\n",
      "loss : 2.772367000579834\n",
      "loss : 2.500154495239258\n",
      "loss : 2.655641555786133\n",
      "loss : 2.70963716506958\n",
      "loss : 2.490868330001831\n",
      "loss : 2.559788942337036\n",
      "loss : 2.493816375732422\n",
      "loss : 2.679422378540039\n",
      "loss : 2.7242753505706787\n",
      "loss : 2.5885140895843506\n",
      "loss : 2.671820878982544\n",
      "loss : 2.4821434020996094\n",
      "loss : 2.622701406478882\n",
      "loss : 2.6544182300567627\n",
      "loss : 2.6048386096954346\n",
      "loss : 2.644951820373535\n",
      "loss : 2.739802837371826\n",
      "loss : 2.525531768798828\n",
      "loss : 2.698002576828003\n",
      "loss : 2.3622145652770996\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I', ed zikd pde bele d oTd ngmycsassWtharo n alep then' wan ind'renst ok\n",
      "Eiforyal wetel-inig Cro lth\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet very good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
