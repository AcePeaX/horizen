{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention embedding table\n",
    "\n",
    "Adding route to custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "dirname = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts/lib\"))\n",
    "sys.path.append(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from utils.compile import compileFolder\n",
    "from utils.tokenizer import CharTokenizer, END_CHAR\n",
    "from utils.datasets import TextChunksDataset, split_dataset, get_batch, estimate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module helps to quickly save the weights and load them\n",
    "from transformers import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max block size (also known as max context) [in tokens]\n",
    "block_size = 16\n",
    "\n",
    "# How much does the test/validation set represent of the total data\n",
    "test_train_split_ratio = 0.1\n",
    "\n",
    "# Number of embedding\n",
    "n_embd = 16\n",
    "\n",
    "# Device (gpu or cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "raw_data = compileFolder('tate')\n",
    "\n",
    "# Creating the tokenizer\n",
    "tokenizer = CharTokenizer(raw_data)\n",
    "\n",
    "# Tokenizing and creating the dataset object\n",
    "data = TextChunksDataset(raw_data, block_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the self attention block\n",
    "\n",
    "We take almost the same structure as the base-embedding structure\n",
    "\n",
    "> Note: starting from now, we're going to use `cuda` when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int, n_embd, context_size=None):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"y=Ts-R8/TugilI'8eL:hRvODUniEJXM!T*iFt dr03?cDh'r-?oJX?DieJ7\"2MX0cM!f3TbFyZX?-zNWn p%O'qKXa3yZ!B'9!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(89)\n",
    "m = BigramLanguageModel(train_data, n_embd)\n",
    "m.to(device)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is random characters\n",
    "\n",
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# verbose\n",
    "show_loss_each_epoch = 500\n",
    "\n",
    "def train(optimizer, num_epochs=num_epochs, loss_verbose_interval=show_loss_each_epoch):\n",
    "    for steps in range(num_epochs):\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(train_data, batch_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (steps+1)%loss_verbose_interval==0:\n",
    "            losses = estimate_loss(m, train_data, test_data, batch_size=batch_size)\n",
    "            print(f\"step {steps+1}: train loss {losses ['train']:.4f}, val loss {losses ['val']:.4f}\")\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 2.8155, val loss 2.8754\n",
      "step 1000: train loss 2.6120, val loss 2.6564\n",
      "step 1500: train loss 2.5659, val loss 2.6120\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dinestowese s tpe tothin igrst yo t jeine\n",
      "AUp, meat\n",
      "\n",
      "\n",
      "\n",
      "s. n aacy th\n",
      "FAye thande.ov? t, tonce i k oke\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still the same result as the base model\n",
    "\n",
    "\n",
    "## The mathematical trick to self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example\n",
    "\n",
    "torch.manual_seed(198)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3597,  0.1501],\n",
       "        [ 0.3383,  0.7864],\n",
       "        [ 0.3464,  0.5391],\n",
       "        [ 0.0438,  0.4631],\n",
       "        [ 0.0989,  0.1779],\n",
       "        [ 0.2658,  0.2987],\n",
       "        [-0.0105,  0.2283],\n",
       "        [ 0.0798,  0.2066]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b, t] = the mean of x[b, i] with i<=t\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second version (using Softmax)\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3597,  0.1501],\n",
       "        [ 0.3383,  0.7864],\n",
       "        [ 0.3464,  0.5391],\n",
       "        [ 0.0438,  0.4631],\n",
       "        [ 0.0989,  0.1779],\n",
       "        [ 0.2658,  0.2987],\n",
       "        [-0.0105,  0.2283],\n",
       "        [ 0.0798,  0.2066]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the same tensor\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same result\n",
    "\n",
    "### Implementing an attention head\n",
    "Now we're going to have a third version: the attention head.\n",
    "\n",
    "We introduce first this new parameter: the head size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head size\n",
    "head_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third version\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4099, 0.5901, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3780, 0.2279, 0.3942, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1302, 0.5305, 0.1159, 0.2234, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2031, 0.0725, 0.2210, 0.0464, 0.4569, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1195, 0.0402, 0.1308, 0.3458, 0.3329, 0.0308, 0.0000, 0.0000],\n",
       "        [0.0555, 0.5743, 0.0458, 0.0614, 0.0072, 0.2289, 0.0269, 0.0000],\n",
       "        [0.1148, 0.0417, 0.1248, 0.1055, 0.2782, 0.0637, 0.1468, 0.1245]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the weights are not uniform in a row, rather have different values.\n",
    "\n",
    "So we've implemented the keys, the queries, now we'll implement the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_size, bias=False) # Same linear structure as the key and query linear models\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2252,  0.0816, -0.0861, -0.0980, -0.1490, -0.0824, -0.2695,  0.0722,\n",
       "         0.1332, -0.1667, -0.0388, -0.2477, -0.0050,  0.1838, -0.0065,  0.1206],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0,0] # The value of the first token for the first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variance stability purposes, `wei` tensor needs to be divided by $\\sqrt{\\text{head\\_size}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * (head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `Head` class that implements a single head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # Compute attention score ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v     # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out        # (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's redefine `BigramLanguageModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int, n_embd, context_size=None, head_size=16):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd, n_embd, vocab_size)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":TgNh$mITng6P:6r*lv TLFVJNvb\n",
      "PCe>UhPe:1B:vB$'S'toLhTF =X0JV.>6->IIEClIyEuK0IVz:hE*!hTmRlh1Dg=XVEG\n",
      "Y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = BigramLanguageModel(train_data, n_embd)\n",
    "m.to(device)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 3.7974, val loss 3.8229\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n.w et.$HL5,wnU\n",
      " c zle yiw atwe8i yyuneuvv%rsouixursn iet   o et \n",
      "i\n",
      "pJon\n",
      " ioebv7a tma Dna pttcetdQdf\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet very good!\n",
    "\n",
    "### Let's dive into multiple head attention\n",
    "\n",
    "Basically, we will have multiple heads that we're going to concatenate into one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention (nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads: int, n_embd: int, head_size: int, block_size: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's redefine our `BigramLanguageModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int, n_embd, context_size=None, head_size=16, num_heads=4):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, n_embd, n_embd//num_heads, vocab_size)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        x = self.sa_heads(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JnMLU$F zGqHHF7,e27M5d0DyauP.RH-YB?NBAnni,G67*ti-%i>rf:p422QBB>T4oNU\n",
      "AOwy-BUk liFD8rSf3%5Cja:iHyyJf\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(train_data, n_embd, num_heads=4)\n",
    "m.to(device)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 3.7187, val loss 3.7363\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wyPjuhV\n",
      "IuE=6!0DsX jpp4Ofd>px2prd9*t7vlulyoee>obmk8v dDuBe0VGwBMwJCV,x'ZhfQVPn MJ*J*OQF?Bj43s…gqM?rb\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, started looking like something. We're very close to the goal!\n",
    "\n",
    "### Let's add the feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reimplement again our `BigramLanguageModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int, n_embd, context_size=None, head_size=16, num_heads=4):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
    "        self.sa_heads = MultiHeadAttention(num_heads, n_embd, n_embd//num_heads, vocab_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f2…o?ZeLaW=LwR!H'O…>…5l/TmM2J\n",
      "veB>8'K7?OSe>70Wj0K4T>'8?xLZu1>WSdV\n",
      "zfx=sxFvJ:gZ62laVMN/N373ufJtd37QE5\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(train_data, n_embd, num_heads=4)\n",
    "m.to(device)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: for quick execution purposes, only a small number of epochs have been put here by default, to enable quick execution. Please note that you should at least have 25000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 3.1368, val loss 3.2254\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cs?g es wt Fe buhp:Wato yoo  h.ehtimsderra d \n",
      "7hg ra sehbh cootottdLW f\n",
      "eEnsa  Oedio \n",
      "fGseeh.s wzino\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing **The Block** class (that we put in a sequence multiple times)\n",
    "\n",
    "For a better optimisation, we will also redefine `MultiHeadAttention` and `FeedForward` by adding a projection layer.\n",
    "We also add `Dropout` layers everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int, dropout=0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "        # Compute attention score ('affinities')\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size**-0.5) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v     # (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "        return out        # (B, T, C)\n",
    "\n",
    "class MultiHeadAttention (nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads: int, n_embd: int, head_size: int, block_size: int, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_heads, block_size, dropout=0.2):\n",
    "        # n_embd: embedding dimension, n_heads: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, n_embd, head_size, block_size, dropout=dropout)\n",
    "        self.ffwd = FeedForward(n_embd, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reimplementing our `BigramLanguageModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(Module):\n",
    "    def __init__(self, vocab_size: int, n_embd, n_layers, context_size=None, head_size=16, n_heads=4, dropout=0.2):\n",
    "        \"\"\"\n",
    "        If vocab_size is a Dataset with context_length, then no need to specify context_size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if context_size==None:\n",
    "            if type(vocab_size)==TextChunksDataset:\n",
    "                context_size = vocab_size.context_length\n",
    "            else:\n",
    "                raise Exception(\"You need to specify the context length\")\n",
    "        self.block_size = context_size\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_heads,self.block_size, dropout=dropout),\n",
    "            Block(n_embd, n_heads,self.block_size, dropout=dropout),\n",
    "            Block(n_embd, n_heads,self.block_size, dropout=dropout),\n",
    "            nn.LayerNorm(n_embd),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = tok_embd + pos_embd # (B,T,C)\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx[:,-self.block_size:])\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$!Va…whj%27vaD%qcR5PTMyXFA0zB5v8V7yk/En?z!Ik6Nf%G\"L…9!\"Hm=R!YLhI5M2m3c3KXGYvU\"mYx!'Ccy1WuKesNO\n",
      "IY=K\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(train_data, n_embd, n_layers=3, n_heads=4, head_size=16)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=3*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 2.7266, val loss 2.7741\n",
      "step 2000: train loss 2.5372, val loss 2.6025\n",
      "step 3000: train loss 2.4203, val loss 2.5030\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, 120, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "$ oniser bun heatyx thinleale t I q aory dko t me fo I2ongot i sisriith\n",
      "pisthe wout be deyenou'pns weppy a t eave s he c yoise. ngho ingicong indod lap maotepat wel y yo IHandoout t chitssreo\n",
      "!t'rely\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=200)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoCompletePrint(model, text, max_tokens = 300, step=1):\n",
    "    idx = tokenizer.encode(text)\n",
    "    i = len(idx)\n",
    "    print(text, end='')\n",
    "    idx = torch.reshape(idx, (1, len(idx)))\n",
    "    for j in range(0, max_tokens, step):\n",
    "        res = m.generate(idx= idx, max_new_tokens=2)\n",
    "        print(tokenizer.decodeText(res[0][i + j: i+j+step]), end='')\n",
    "        idx = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am the best I'the thhengy ban ve ate he ciman iyoull ame bngora ipere y? herl\n",
      "atousear\n",
      "merppe bapine t' y tha to icoutsu alee'ct t maro madinillseclle s focon owyoue llndot on'sea erean \n",
      "ouamellrve the shonde swiko a I gucethidopoe t d yndou'courels ow ang, thancrlyontend ouitl\n",
      "nel at\n",
      "acany oufa t hlene on ceingom andisfHhe ald Sldogd woumey wou'en ciend syof lneth w thoudo ag o7\n",
      "adomm, d Ke mincond las de o"
     ]
    }
   ],
   "source": [
    "autoCompletePrint(None, \"I am the best\", max_tokens=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding these classes to the libs\n",
    "\n",
    "You can now import directly these files as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BasicSelfAttentionLanguageModel\n",
    "\n",
    "from models.utils import Head, MultiHeadAttention, FeedForward, Block"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
