{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base embedding table\n",
    "\n",
    "Adding route to custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "dirname = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts/lib\"))\n",
    "sys.path.append(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max block size (also known as max context) [in tokens]\n",
    "block_size = 8\n",
    "\n",
    "# How much does the test/validation set represent of the total data\n",
    "test_train_split_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.compile import compileFolder\n",
    "from utils.tokenizer import CharTokenizer, END_CHAR\n",
    "from utils.datasets import TextChunksDataset, split_dataset, get_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "raw_data = compileFolder('tate')\n",
    "\n",
    "# Creating the tokenizer\n",
    "tokenizer = CharTokenizer(raw_data)\n",
    "\n",
    "# Tokenizing and creating the dataset object\n",
    "data = TextChunksDataset(raw_data, block_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 80])\n",
      "tensor(5.1642, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(89)\n",
    "m = BigramLanguageModel(train_data)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(out[0].shape)\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3!!UvlMHTU.Sf=NBelz89HkL:Tz!HwuF=cim>.B/:/:8kFrPuKcNF%3H5fVea-81\"2ySkTZpaHJkDgqjBAghxaq=1'3XUC4,eR Y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is random characters\n",
    "\n",
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a function to estimate training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, x_train : TextChunksDataset, x_val : TextChunksDataset, eval_iterations = 50 ,batch_size=32):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for label, data in zip(['train','val'],[x_train, x_val]):\n",
    "        losses= torch.zeros(eval_iterations)\n",
    "        for k in range(eval_iterations):\n",
    "            X, Y = get_batch(data, batch_size)\n",
    "            _, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[label] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 99: train loss 4.8407, val loss 4.8641\n",
      "step 199: train loss 4.7100, val loss 4.7254\n",
      "step 299: train loss 4.6005, val loss 4.5998\n",
      "step 399: train loss 4.4951, val loss 4.4947\n",
      "step 499: train loss 4.3807, val loss 4.3918\n",
      "step 599: train loss 4.2687, val loss 4.2840\n",
      "step 699: train loss 4.1695, val loss 4.1794\n",
      "step 799: train loss 4.0673, val loss 4.1025\n",
      "step 899: train loss 3.9953, val loss 4.0208\n",
      "step 999: train loss 3.9112, val loss 3.9341\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "\n",
    "# verbose\n",
    "show_loss_each_epoch = 100\n",
    "\n",
    "for steps in range(num_epochs):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(train_data, batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (steps+1)%show_loss_each_epoch==0:\n",
    "        losses = estimate_loss(m, train_data, test_data, batch_size=batch_size)\n",
    "        print(f\"step {steps}: train loss {losses ['train']:.4f}, val loss {losses ['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ND%IuyND=%-/u'Gam6w Hl74.=e.>do=bsows0B'/\n",
      "CHCR>$hzN/GoX2CpRdi:/GGYeeefRUR93BMW-X%5Râ€¦XPqYa\n",
      "1Rzg24*xt\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** : So far, this model only looks at the previous character, so there is no context\n",
    "\n",
    "To use this model directly, you can import it from `transformers.educational`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.educational import BigramLanguageBaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to use the `estimate_loss` function directly, import it from `utils.dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import estimate_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
