{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base embedding table\n",
    "\n",
    "Adding route to custom libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "dirname = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"scripts/lib\"))\n",
    "sys.path.append(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max block size (also known as max context) [in tokens]\n",
    "block_size = 8\n",
    "\n",
    "# How much does the test/validation set represent of the total data\n",
    "test_train_split_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.compile import compileFolder\n",
    "from utils.tokenizer import CharTokenizer, END_CHAR\n",
    "from utils.datasets import TextChunksDataset, split_dataset, get_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "raw_data = compileFolder('tate')\n",
    "\n",
    "# Creating the tokenizer\n",
    "tokenizer = CharTokenizer(raw_data)\n",
    "\n",
    "# Tokenizing and creating the dataset object\n",
    "data = TextChunksDataset(raw_data, block_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int | CharTokenizer | TextChunksDataset):\n",
    "        super().__init__()\n",
    "\n",
    "        if type(vocab_size)==TextChunksDataset:\n",
    "            vocab_size=len(vocab_size.tokenizer)\n",
    "        elif type(vocab_size)==CharTokenizer:\n",
    "            vocab_size=len(vocab_size)\n",
    "        # each token has a probability distribution of appearing depending on the last token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:,-1,:]\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled text to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 80])\n",
      "tensor(4.8054, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(89)\n",
    "m = BigramLanguageModel(train_data)\n",
    "xb, yb = train_data[:10]\n",
    "out = m(xb, yb)\n",
    "print(out[0].shape)\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3!!UvlMHTU.Sf=NBelz89HkL:Tz!HwuF=cim>.B/:/:8kFrPuKcNF%3H5fVea-81\"2ySkTZpaHJkDgqjBAghxaq=1'3XUC4,eR Y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is random characters\n",
    "\n",
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 4.918609619140625\n",
      "loss : 4.963029384613037\n",
      "loss : 4.9680352210998535\n",
      "loss : 4.9516215324401855\n",
      "loss : 4.852038383483887\n",
      "loss : 4.976519584655762\n",
      "loss : 4.953675746917725\n",
      "loss : 4.949826717376709\n",
      "loss : 4.8634185791015625\n",
      "loss : 4.948612213134766\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "# verbose\n",
    "show_loss_each_epoch = 10\n",
    "\n",
    "for steps in range(num_epochs):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(train_data, batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (steps+1)%show_loss_each_epoch==0:\n",
    "        print('loss :',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xL10UJvyQ5N8Rm:cPKlPz%Fj!8FL uagbvvIX,YZKcl!PJlr'W-DehzJlQDfOfcmpHFv=!%vQK.l soMâ€¦T1Vg6C,%qDmI:!PN2H*\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decodeText(m.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** : So far, this model only looks at the previous character, so there is no context\n",
    "\n",
    "To use this model directly, you can import it from `transformers.educational`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.educational import BigramLanguageBaseModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
